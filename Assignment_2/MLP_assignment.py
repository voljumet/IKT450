import numpy
import torch
import torch.nn as nn
import torch.optim as optim

#
# crim	tax	rm	age	ptratio	medv
training_dataset = """
1   0.49   0.29   0.48   0.50   0.56   0.24   0.35   1.0
2   0.07   0.40   0.48   0.50   0.54   0.35   0.44   1.0
3   0.56   0.40   0.48   0.50   0.49   0.37   0.46   1.0
4   0.59   0.49   0.48   0.50   0.52   0.45   0.36   1.0
5   0.23   0.32   0.48   0.50   0.55   0.25   0.35   1.0
6   0.67   0.39   0.48   0.50   0.36   0.38   0.46   1.0
7   0.29   0.28   0.48   0.50   0.44   0.23   0.34   1.0
8   0.21   0.34   0.48   0.50   0.51   0.28   0.39   1.0
9   0.20   0.44   0.48   0.50   0.46   0.51   0.57   1.0
10   0.42   0.40   0.48   0.50   0.56   0.18   0.30   1.0
11   0.42   0.24   0.48   0.50   0.57   0.27   0.37   1.0
12   0.25   0.48   0.48   0.50   0.44   0.17   0.29   1.0
13   0.39   0.32   0.48   0.50   0.46   0.24   0.35   1.0
14   0.51   0.50   0.48   0.50   0.46   0.32   0.35   1.0
15   0.22   0.43   0.48   0.50   0.48   0.16   0.28   1.0
16   0.25   0.40   0.48   0.50   0.46   0.44   0.52   1.0
17   0.34   0.45   0.48   0.50   0.38   0.24   0.35   1.0
18   0.44   0.27   0.48   0.50   0.55   0.52   0.58   1.0
19   0.23   0.40   0.48   0.50   0.39   0.28   0.38   1.0
20   0.41   0.57   0.48   0.50   0.39   0.21   0.32   1.0
21   0.40   0.45   0.48   0.50   0.38   0.22   0.00   1.0
22   0.31   0.23   0.48   0.50   0.73   0.05   0.14   1.0
23   0.51   0.54   0.48   0.50   0.41   0.34   0.43   1.0
24   0.30   0.16   0.48   0.50   0.56   0.11   0.23   1.0
25   0.36   0.39   0.48   0.50   0.48   0.22   0.23   1.0
26   0.29   0.37   0.48   0.50   0.48   0.44   0.52   1.0
27   0.25   0.40   0.48   0.50   0.47   0.33   0.42   1.0
28   0.21   0.51   0.48   0.50   0.50   0.32   0.41   1.0
29   0.43   0.37   0.48   0.50   0.53   0.35   0.44   1.0
30   0.43   0.39   0.48   0.50   0.47   0.31   0.41   1.0
31   0.53   0.38   0.48   0.50   0.44   0.26   0.36   1.0
32   0.34   0.33   0.48   0.50   0.38   0.35   0.44   1.0
33   0.56   0.51   0.48   0.50   0.34   0.37   0.46   1.0
34   0.40   0.29   0.48   0.50   0.42   0.35   0.44   1.0
35   0.24   0.35   0.48   0.50   0.31   0.19   0.31   1.0
36   0.36   0.54   0.48   0.50   0.41   0.38   0.46   1.0
37   0.29   0.52   0.48   0.50   0.42   0.29   0.39   1.0
38   0.65   0.47   0.48   0.50   0.59   0.30   0.40   1.0
39   0.32   0.42   0.48   0.50   0.35   0.28   0.38   1.0
40   0.38   0.46   0.48   0.50   0.48   0.22   0.29   1.0
41   0.33   0.45   0.48   0.50   0.52   0.32   0.41   1.0
42   0.30   0.37   0.48   0.50   0.59   0.41   0.49   1.0
43   0.40   0.50   0.48   0.50   0.45   0.39   0.47   1.0
44   0.28   0.38   0.48   0.50   0.50   0.33   0.42   1.0
45   0.61   0.45   0.48   0.50   0.48   0.35   0.41   1.0
46   0.17   0.38   0.48   0.50   0.45   0.42   0.50   1.0
47   0.44   0.35   0.48   0.50   0.55   0.55   0.61   1.0
48   0.43   0.40   0.48   0.50   0.39   0.28   0.39   1.0
49   0.42   0.35   0.48   0.50   0.58   0.15   0.27   1.0
50   0.23   0.33   0.48   0.50   0.43   0.33   0.43   1.0
51   0.37   0.52   0.48   0.50   0.42   0.42   0.36   1.0
52   0.29   0.30   0.48   0.50   0.45   0.03   0.17   1.0
53   0.22   0.36   0.48   0.50   0.35   0.39   0.47   1.0
54   0.23   0.58   0.48   0.50   0.37   0.53   0.59   1.0
55   0.47   0.47   0.48   0.50   0.22   0.16   0.26   1.0
56   0.54   0.47   0.48   0.50   0.28   0.33   0.42   1.0
57   0.51   0.37   0.48   0.50   0.35   0.36   0.45   1.0
58   0.40   0.35   0.48   0.50   0.45   0.33   0.42   1.0
59   0.44   0.34   0.48   0.50   0.30   0.33   0.43   1.0
60   0.42   0.38   0.48   0.50   0.54   0.34   0.43   1.0
61   0.44   0.56   0.48   0.50   0.50   0.46   0.54   1.0
62   0.52   0.36   0.48   0.50   0.41   0.28   0.38   1.0
63   0.36   0.41   0.48   0.50   0.48   0.47   0.54   1.0
64   0.18   0.30   0.48   0.50   0.46   0.24   0.35   1.0
65   0.47   0.29   0.48   0.50   0.51   0.33   0.43   1.0
66   0.24   0.43   0.48   0.50   0.54   0.52   0.59   1.0
67   0.25   0.37   0.48   0.50   0.41   0.33   0.42   1.0
68   0.52   0.57   0.48   0.50   0.42   0.47   0.54   1.0
69   0.25   0.37   0.48   0.50   0.43   0.26   0.36   1.0
70   0.35   0.48   0.48   0.50   0.56   0.40   0.48   1.0
71   0.26   0.26   0.48   0.50   0.34   0.25   0.35   1.0
72   0.44   0.51   0.48   0.50   0.47   0.26   0.36   1.0
73   0.37   0.50   0.48   0.50   0.42   0.36   0.45   1.0
74   0.44   0.42   0.48   0.50   0.42   0.25   0.20   1.0
75   0.24   0.43   0.48   0.50   0.37   0.28   0.38   1.0
76   0.42   0.30   0.48   0.50   0.48   0.26   0.36   1.0
77   0.48   0.42   0.48   0.50   0.45   0.25   0.35   1.0
78   0.41   0.48   0.48   0.50   0.51   0.44   0.51   1.0
79   0.44   0.28   0.48   0.50   0.43   0.27   0.37   1.0
80   0.29   0.41   0.48   0.50   0.48   0.38   0.46   1.0
81   0.34   0.28   0.48   0.50   0.41   0.35   0.44   1.0
82   0.41   0.43   0.48   0.50   0.45   0.31   0.41   1.0
83   0.29   0.47   0.48   0.50   0.41   0.23   0.34   1.0
84   0.34   0.55   0.48   0.50   0.58   0.31   0.41   1.0
85   0.36   0.56   0.48   0.50   0.43   0.45   0.53   1.0
86   0.40   0.46   0.48   0.50   0.52   0.49   0.56   1.0
87   0.50   0.49   0.48   0.50   0.49   0.46   0.53   1.0
88   0.52   0.44   0.48   0.50   0.37   0.36   0.42   1.0
89   0.50   0.51   0.48   0.50   0.27   0.23   0.34   1.0
90   0.53   0.42   0.48   0.50   0.16   0.29   0.39   1.0
91   0.34   0.46   0.48   0.50   0.52   0.35   0.44   1.0
92   0.40   0.42   0.48   0.50   0.37   0.27   0.27   1.0
93   0.41   0.43   0.48   0.50   0.50   0.24   0.25   1.0
94   0.30   0.45   0.48   0.50   0.36   0.21   0.32   1.0
95   0.31   0.47   0.48   0.50   0.29   0.28   0.39   1.0
96   0.64   0.76   0.48   0.50   0.45   0.35   0.38   1.0
97   0.35   0.37   0.48   0.50   0.30   0.34   0.43   1.0
98   0.57   0.54   0.48   0.50   0.37   0.28   0.33   1.0
99   0.65   0.55   0.48   0.50   0.34   0.37   0.28   1.0
100   0.51   0.46   0.48   0.50   0.58   0.31   0.41   1.0
101   0.38   0.40   0.48   0.50   0.63   0.25   0.35   1.0
102   0.24   0.57   0.48   0.50   0.63   0.34   0.43   1.0
103   0.38   0.26   0.48   0.50   0.54   0.16   0.28   1.0
104   0.33   0.47   0.48   0.50   0.53   0.18   0.29   1.0
105   0.24   0.34   0.48   0.50   0.38   0.30   0.40   1.0
106   0.26   0.50   0.48   0.50   0.44   0.32   0.41   1.0
107   0.44   0.49   0.48   0.50   0.39   0.38   0.40   1.0
108   0.43   0.32   0.48   0.50   0.33   0.45   0.52   1.0
109   0.49   0.43   0.48   0.50   0.49   0.30   0.40   1.0
110   0.47   0.28   0.48   0.50   0.56   0.20   0.25   1.0
111   0.32   0.33   0.48   0.50   0.60   0.06   0.20   1.0
112   0.34   0.35   0.48   0.50   0.51   0.49   0.56   1.0
113   0.35   0.34   0.48   0.50   0.46   0.30   0.27   1.0
114   0.38   0.30   0.48   0.50   0.43   0.29   0.39   1.0
115   0.38   0.44   0.48   0.50   0.43   0.20   0.31   1.0
116   0.41   0.51   0.48   0.50   0.58   0.20   0.31   1.0
117   0.34   0.42   0.48   0.50   0.41   0.34   0.43   1.0
118   0.51   0.49   0.48   0.50   0.53   0.14   0.26   1.0
119   0.25   0.51   0.48   0.50   0.37   0.42   0.50   1.0
120   0.29   0.28   0.48   0.50   0.50   0.42   0.50   1.0
121   0.25   0.26   0.48   0.50   0.39   0.32   0.42   1.0
122   0.24   0.41   0.48   0.50   0.49   0.23   0.34   1.0
123   0.17   0.39   0.48   0.50   0.53   0.30   0.39   1.0
124   0.04   0.31   0.48   0.50   0.41   0.29   0.39   1.0
125   0.61   0.36   0.48   0.50   0.49   0.35   0.44   1.0
126   0.34   0.51   0.48   0.50   0.44   0.37   0.46   1.0
127   0.28   0.33   0.48   0.50   0.45   0.22   0.33   1.0
128   0.40   0.46   0.48   0.50   0.42   0.35   0.44   1.0
129   0.23   0.34   0.48   0.50   0.43   0.26   0.37   1.0
130   0.37   0.44   0.48   0.50   0.42   0.39   0.47   1.0
131   0.00   0.38   0.48   0.50   0.42   0.48   0.55   1.0
132   0.39   0.31   0.48   0.50   0.38   0.34   0.43   1.0
133   0.30   0.44   0.48   0.50   0.49   0.22   0.33   1.0
134   0.27   0.30   0.48   0.50   0.71   0.28   0.39   1.0
135   0.17   0.52   0.48   0.50   0.49   0.37   0.46   1.0
136   0.36   0.42   0.48   0.50   0.53   0.32   0.41   1.0
137   0.30   0.37   0.48   0.50   0.43   0.18   0.30   1.0
138   0.26   0.40   0.48   0.50   0.36   0.26   0.37   1.0
139   0.40   0.41   0.48   0.50   0.55   0.22   0.33   1.0
140   0.22   0.34   0.48   0.50   0.42   0.29   0.39   1.0
141   0.44   0.35   0.48   0.50   0.44   0.52   0.59   1.0
142   0.27   0.42   0.48   0.50   0.37   0.38   0.43   1.0
143   0.16   0.43   0.48   0.50   0.54   0.27   0.37   1.0
144   0.06   0.61   0.48   0.50   0.49   0.92   0.37   0.0
145   0.44   0.52   0.48   0.50   0.43   0.47   0.54   0.0
146   0.63   0.47   0.48   0.50   0.51   0.82   0.84   0.0
147   0.23   0.48   0.48   0.50   0.59   0.88   0.89   0.0
148   0.34   0.49   0.48   0.50   0.58   0.85   0.80   0.0
149   0.43   0.40   0.48   0.50   0.58   0.75   0.78   0.0
150   0.46   0.61   0.48   0.50   0.48   0.86   0.87   0.0
151   0.27   0.35   0.48   0.50   0.51   0.77   0.79   0.0
152   0.52   0.39   0.48   0.50   0.65   0.71   0.73   0.0
153   0.29   0.47   0.48   0.50   0.71   0.65   0.69   0.0
154   0.55   0.47   0.48   0.50   0.57   0.78   0.80   0.0
155   0.12   0.67   0.48   0.50   0.74   0.58   0.63   0.0
156   0.40   0.50   0.48   0.50   0.65   0.82   0.84   0.0
157   0.73   0.36   0.48   0.50   0.53   0.91   0.92   0.0
158   0.84   0.44   0.48   0.50   0.48   0.71   0.74   0.0
159   0.48   0.45   0.48   0.50   0.60   0.78   0.80   0.0
160   0.54   0.49   0.48   0.50   0.40   0.87   0.88   0.0
161   0.48   0.41   0.48   0.50   0.51   0.90   0.88   0.0
162   0.50   0.66   0.48   0.50   0.31   0.92   0.92   0.0
163   0.72   0.46   0.48   0.50   0.51   0.66   0.70   0.0
164   0.47   0.55   0.48   0.50   0.58   0.71   0.75   0.0
165   0.33   0.56   0.48   0.50   0.33   0.78   0.80   0.0
166   0.64   0.58   0.48   0.50   0.48   0.78   0.73   0.0
167   0.54   0.57   0.48   0.50   0.56   0.81   0.83   0.0
168   0.47   0.59   0.48   0.50   0.52   0.76   0.79   0.0
169   0.63   0.50   0.48   0.50   0.59   0.85   0.86   0.0
170   0.49   0.42   0.48   0.50   0.53   0.79   0.81   0.0
171   0.31   0.50   0.48   0.50   0.57   0.84   0.85   0.0
172   0.74   0.44   0.48   0.50   0.55   0.88   0.89   0.0
173   0.33   0.45   0.48   0.50   0.45   0.88   0.89   0.0
174   0.45   0.40   0.48   0.50   0.61   0.74   0.77   0.0
175   0.71   0.40   0.48   0.50   0.71   0.70   0.74   0.0
176   0.50   0.37   0.48   0.50   0.66   0.64   0.69   0.0
177   0.66   0.53   0.48   0.50   0.59   0.66   0.66   0.0
178   0.60   0.61   0.48   0.50   0.54   0.67   0.71   0.0
179   0.83   0.37   0.48   0.50   0.61   0.71   0.74   0.0
180   0.34   0.51   0.48   0.50   0.67   0.90   0.90   0.0
181   0.63   0.54   0.48   0.50   0.65   0.79   0.81   0.0
182   0.70   0.40   0.48   0.50   0.56   0.86   0.83   0.0
183   0.60   0.50   1.00   0.50   0.54   0.77   0.80   0.0
184   0.16   0.51   0.48   0.50   0.33   0.39   0.48   0.0
185   0.74   0.70   0.48   0.50   0.66   0.65   0.69   0.0
186   0.20   0.46   0.48   0.50   0.57   0.78   0.81   0.0
187   0.89   0.55   0.48   0.50   0.51   0.72   0.76   0.0
188   0.70   0.46   0.48   0.50   0.56   0.78   0.73   0.0
189   0.12   0.43   0.48   0.50   0.63   0.70   0.74   0.0
190   0.61   0.52   0.48   0.50   0.54   0.67   0.52   0.0
191   0.33   0.37   0.48   0.50   0.46   0.65   0.69   0.0
192   0.63   0.65   0.48   0.50   0.66   0.67   0.71   0.0
193   0.41   0.51   0.48   0.50   0.53   0.75   0.78   0.0
194   0.34   0.67   0.48   0.50   0.52   0.76   0.79   0.0
195   0.58   0.34   0.48   0.50   0.56   0.87   0.81   0.0
196   0.59   0.56   0.48   0.50   0.55   0.80   0.82   0.0
197   0.51   0.40   0.48   0.50   0.57   0.62   0.67   0.0
198   0.50   0.57   0.48   0.50   0.71   0.61   0.66   0.0
199   0.60   0.46   0.48   0.50   0.45   0.81   0.83   0.0
200   0.37   0.47   0.48   0.50   0.39   0.76   0.79   0.0
201   0.58   0.55   0.48   0.50   0.57   0.70   0.74   0.0
202   0.36   0.47   0.48   0.50   0.51   0.69   0.72   0.0
203   0.39   0.41   0.48   0.50   0.52   0.72   0.75   0.0
204   0.35   0.51   0.48   0.50   0.61   0.71   0.74   0.0
205   0.31   0.44   0.48   0.50   0.50   0.79   0.82   0.0
206   0.61   0.66   0.48   0.50   0.46   0.87   0.88   0.0
207   0.48   0.49   0.48   0.50   0.52   0.77   0.71   0.0
208   0.11   0.50   0.48   0.50   0.58   0.72   0.68   0.0
209   0.31   0.36   0.48   0.50   0.58   0.94   0.94   0.0
210   0.68   0.51   0.48   0.50   0.71   0.75   0.78   0.0
211   0.69   0.39   0.48   0.50   0.57   0.76   0.79   0.0
212   0.52   0.54   0.48   0.50   0.62   0.76   0.79   0.0
213   0.46   0.59   0.48   0.50   0.36   0.76   0.23   0.0
214   0.36   0.45   0.48   0.50   0.38   0.79   0.17   0.0
215   0.00   0.51   0.48   0.50   0.35   0.67   0.44   0.0
216   0.10   0.49   0.48   0.50   0.41   0.67   0.21   0.0
217   0.30   0.51   0.48   0.50   0.42   0.61   0.34   0.0
218   0.61   0.47   0.48   0.50   0.00   0.80   0.32   0.0
219   0.63   0.75   0.48   0.50   0.64   0.73   0.66   0.0
220   0.71   0.52   0.48   0.50   0.64   1.00   0.99   0.0
"""
#
# with open("ecoli.data", "r") as training_dataset:
#     for line in training_dataset:
#         print (line.split("t"))

training_dataset = [[float(f) for f in i.split("   ")] for i in  training_dataset.strip().split("\n")]

numpy.random.shuffle(training_dataset)
# #Making into a binary classifier
# training_dataset = [row[:-1]+[0 if row[-1]<25 else 1] for row in training_dataset]
# training_dataset =training_dataset*1000

#print(training_dataset)
X = torch.Tensor([i[0:8] for i in training_dataset])
Y = torch.Tensor([i[8] for i in training_dataset])
#
class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.fc1 = nn.Linear(8,2)
        self.fc2 = nn.Linear(2,1)

    def forward(self,x):
        x = self.fc1(x)
        x = torch.sigmoid(x)
        x = self.fc2(x)
        x = torch.sigmoid(x)
        return x
#
model = Net()
# print(model)
#
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)
#
allloss = []
#
for epoch in range(135):
    print(epoch)
    outputs = model(X)
    loss = criterion(outputs,Y)
    loss.backward()
    optimizer.step()
    allloss.append(loss.item())
import pdb;pdb.set_trace()


import matplotlib.pyplot as plt
# plt.plot(allloss)
# plt.show()

# print(list(model.parameters()))

# import sys
# sys.exit(0)


#From scratch
weights = [-0.1, 0.20, -0.23, -0.1, 0.20, -0.23, -0.1, 0.20, -0.23, 0.24, 0.1337, -0.420, 0.12]

import math
def sigmoid(z):
    if(z<-100):
        return 0
    if(z>100):
        return 1
    return 1.0/math.exp(-z)

def firstLayer(row,weights):
    activation_1 = weights[0]*1 # w0
    activation_1 += weights[1]*row[0]
    activation_1 += weights[2]*row[1]
    activation_1 += weights[3]*row[2]
    activation_1 += weights[4]*row[3]

    activation_2 = weights[5]* 1 # w5
    activation_2 += weights[6] * row[4]
    activation_2 += weights[7] * row[5]
    activation_2 += weights[8] * row[6]
    activation_2 += weights[9] * row[7]

    return sigmoid(activation_1),sigmoid(activation_2)

def secondLayer(row,weights):
    activation_3 = weights[10]*1 # w10
    activation_3 += weights[11]*row[0]
    activation_3 += weights[12]*row[1]
    return sigmoid(activation_3)

def predict(row,weights):
    input_layer = row
    first_layer = firstLayer(input_layer,weights)
    second_layer = secondLayer(first_layer,weights)
    return second_layer,first_layer


for d in training_dataset:
    print(predict(d,weights)[0],d[-1])   #Prints y_hat and y


allloss = []
def train_weights(train,learningrate,epochs):
    for epoch in range(epochs):
        sum_error = 0.0
        for row in train:
            prediction,first_layer = predict(row[::-1],weights)
            error = (row[-1] - prediction)
            sum_error += error

            #First layer
            weights[0] = weights[0] + learningrate*error # Bias
            weights[1] = weights[1] + learningrate*error*row[0]
            weights[2] = weights[2] + learningrate*error*row[1]
            weights[3] = weights[3] + learningrate*error*row[2]
            weights[4] = weights[4] + learningrate*error*row[3]

            weights[5] = weights[5] + learningrate*error # Bias
            weights[6] = weights[6] + learningrate*error*row[4]
            weights[7] = weights[7] + learningrate*error*row[5]
            weights[8] = weights[8] + learningrate*error*row[6]
            weights[9] = weights[9] + learningrate*error*row[7]

            #Second layer
            weights[10] = weights[10] + learningrate*error # Bias
            weights[11] = weights[11] + learningrate*error*first_layer[0]
            weights[12] = weights[12] + learningrate*error*first_layer[1]

        if((epoch%100==0) or (last_error != sum_error)):
            print("Epoch "+str(epoch) + " Learning rate " + str(learningrate) + " Error " + str(sum_error))
        last_error = sum_error
        allloss.append(last_error)
    return weights

learningrate = 0.0001 #0.00001
epochs = 1000
print("befiore",weights)
train_weights = train_weights(training_dataset, learningrate, epochs)
print("after",weights)

import matplotlib.pyplot as plt
plt.plot(allloss)
plt.show()
